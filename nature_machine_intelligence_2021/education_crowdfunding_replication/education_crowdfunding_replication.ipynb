{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating Education Crowdfunding Results\n",
    "\n",
    "\n",
    "### What is this?\n",
    "This notebook provides step-by-step instructions for replicating the results from our recent study of fairness-accuracy trade-offs on the Education Crowdfunding dataset.\n",
    "\n",
    "### What has already been done?\n",
    "To make reproducing the bias analysis easier as well as provide more visibility into our results, this notebook starts off from a point where the public DonorsChoose dataset has already been injested into a postgres database and a grid of models (described in Supplementary Table 2 of our study) has already been run. We've made available a database dump with both the underlying data and the results of our model runs (see below for instructions on getting it set up).\n",
    "\n",
    "### What if I want to re-run the models from scratch?\n",
    "You can also re-run (or add to) the model grid starting from the public DonorsChoose data using our open-source machine learning pipeline `triage`, which makes it easier to run any model with a `scikit-learn`-style interface over temporal validation sets in a \"top k\" setting. See this repo's readme file for some tips on getting set up as well as pointers to a repo with an example of running triage using these data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import sqlalchemy\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from IPython.display import display \n",
    "from itertools import permutations\n",
    "from jinja2 import Template\n",
    "import dateparser\n",
    "\n",
    "\n",
    "from ohio.ext.numpy import pg_copy_to_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Set Up\n",
    "\n",
    "We've provided an extract of the data in a postgres dump file (postgres 11.10), available at: \n",
    "\n",
    "https://dsapp-public-data-migrated.s3.amazonaws.com/education_crowdfunding_replication.dmp \n",
    "\n",
    "(NOTE: this file is about 16 GB compressed, so may take some time to download depending on your connection). You can load the data into your own postgres server using (you'll need to create the `education_crowdfunding` database first and fill in the host, port, and user below):\n",
    "\n",
    "```bash\n",
    "pg_restore -h {POSTGRES_HOST} -p {POSTGRES_PORT} -d education_crowdfunding -U {POSTGRES_USER} -O -j 8 education_crowdfunding_replication.dmp\n",
    "```\n",
    "\n",
    "Note that the dump file is compressed and you'll need a server with around 500 GB of free disk space.\n",
    "\n",
    "Loading this file will populate your database with several schemas:\n",
    "- **public**: The raw data from donors choose as well as some tables with calculated features and intermediate modeling tables\n",
    "- **model_metadata**: Information about the models we ran, such as model types and hyperparameters (models were run with `triage`, which generates this schema. In `triage` a \"model group\" specifies a type of model and associated hyperparameter values, while a \"model\" is an instantiation of a given model group on a specific temporal validation split). Note that this schema contains information on other model runs with this dataset, in addition to the run used for the current study of fairness-accuracy trade-offs.\n",
    "- **test_results**: Validation set statistics and predictions for the models. Here, `test_results.predictions` contains project-level predicted scores from each model in the grid, while `test_results.evaluations` contains aggregated summary statistics for each model.\n",
    "- **train_results**: Training set statistics for the models, including feature importances.\n",
    "- **features**: Intermediate tables containing calculated features from the `triage` run.\n",
    "- **bias_working**: Intermediate tables from the bias analysis, as well as the mapping table between projects and school poverty levels, `bias_working.entity_demos`.\n",
    "- **bias_results_submitted**: Results of the fairness-accuracy trade-offs from the study as submitted (see below to use these to replicate the figures from the study).\n",
    "- **bias_results**: Empty bias analysis results tables that will be populated by re-running the fairness adjustments (see below for instructions).\n",
    "\n",
    "\n",
    "Finally, to connect to the database, you'll need a yaml file named `db_profile.yaml` in the same directory as this notebook with your connection info:\n",
    "```yaml\n",
    "host: {POSTGRES_HOST}\n",
    "user: {POSTGRES_USER}\n",
    "db: education_crowdfunding\n",
    "pass: {POSTGRES_PASSWORD}\n",
    "port: {POSTGRES_PORT}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('db_profile.yaml') as fd:\n",
    "    config = yaml.full_load(fd)\n",
    "    dburl = sqlalchemy.engine.url.URL(\n",
    "        \"postgresql\",\n",
    "        host=config[\"host\"],\n",
    "        username=config[\"user\"],\n",
    "        database=config[\"db\"],\n",
    "        password=config[\"pass\"],\n",
    "        port=config[\"port\"],\n",
    "    )\n",
    "    engine_donors = sqlalchemy.create_engine(dburl, poolclass=sqlalchemy.pool.QueuePool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a helper function that will pull the results of the various strategies explored in the study. The table `model_adjustment_results_plevel` contains performance statistics for each model type we ran with and without adjusting for recall equity and `composite_results_plevel` contains performance statistics for a stategy that selects the best-performing model for each subgroup separately (we'll look at generating these tables from the raw modeling results as well below). The query below uses four different strategies to choose a model for each temporal validation split based on these performance statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_results(engine, schema='bias_results'):\n",
    "    sql = \"\"\"\n",
    "    WITH mg_rns AS (\n",
    "      SELECT *,\n",
    "             row_number() OVER (PARTITION BY train_end_time, list_size, metric, parameter ORDER BY base_value DESC, base_max_recall_ratio ASC, RANDOM()) AS rn_base,\n",
    "             row_number() OVER (PARTITION BY train_end_time, list_size, metric, parameter ORDER BY adj_value DESC, adj_max_recall_ratio ASC, RANDOM()) AS rn_adj, \n",
    "             row_number() OVER (PARTITION BY train_end_time, list_size, metric, parameter ORDER BY adj_value DESC, adj_max_recall_ratio ASC, RANDOM()) AS rn_multi_adj\n",
    "      FROM {schema}.model_adjustment_results_plevel\n",
    "      WHERE past_train_end_time = train_end_time\n",
    "    )\n",
    "    , base_mgs AS (\n",
    "      SELECT * FROM mg_rns WHERE rn_base = 1\n",
    "    )\n",
    "    , adj_mgs AS (\n",
    "      SELECT * FROM mg_rns WHERE rn_adj = 1\n",
    "    )\n",
    "    , multi_adj_mgs AS (\n",
    "      SELECT * FROM mg_rns WHERE rn_adj = 1\n",
    "    )\n",
    "    -- Simple model selection on last time period, baseline with no recall adjustments\n",
    "    SELECT 'Best Unadjusted Metric - Unadjusted'::VARCHAR(128) AS strategy,\n",
    "           r.train_end_time, r.past_train_end_time,\n",
    "           r.list_size, r.metric, r.parameter,\n",
    "           r.base_value AS value,\n",
    "           r.base_max_recall_ratio AS max_recall_ratio,\n",
    "           r.base_recall_not_highest_to_highest AS recall_not_highest_to_highest,\n",
    "           r.base_frac_not_highest AS frac_not_highest,\n",
    "           r.base_frac_highest AS frac_highest,\n",
    "           r.base_min_score_not_highest AS min_score_not_highest, \n",
    "           r.base_min_score_highest AS min_score_highest, \n",
    "           r.base_mean_score_not_highest AS mean_score_not_highest, \n",
    "           r.base_mean_score_highest AS mean_score_highest,\n",
    "           r.base_precision_not_highest AS precision_not_highest, \n",
    "           r.base_precision_highest AS precision_highest, \n",
    "           r.base_prevalence_not_highest AS prevalence_not_highest, \n",
    "           r.base_prevalence_highest AS prevalence_highest, \n",
    "           r.base_positive_prevalence_not_highest AS positive_prevalence_not_highest, \n",
    "           r.base_positive_prevalence_highest AS positive_prevalence_highest,            \n",
    "           r.base_positive_rate_not_highest AS positive_rate_not_highest, \n",
    "           r.base_positive_rate_highest AS positive_rate_highest\n",
    "    FROM {schema}.model_adjustment_results_plevel r\n",
    "    JOIN base_mgs b\n",
    "      ON r.model_group_id = b.model_group_id\n",
    "      AND r.past_train_end_time = b.train_end_time\n",
    "      AND r.list_size = b.list_size\n",
    "      AND r.metric = b.metric\n",
    "      AND r.parameter = b.parameter\n",
    "    WHERE r.train_end_time > r.past_train_end_time\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Model selection on last time before adjustment, with adjustment applied\n",
    "    SELECT 'Best Unadjusted Metric - Adjusted'::VARCHAR(128) AS strategy,\n",
    "           r.train_end_time, r.past_train_end_time,\n",
    "           r.list_size, r.metric, r.parameter,\n",
    "           r.adj_value AS value,\n",
    "           r.adj_max_recall_ratio AS max_recall_ratio,\n",
    "           r.adj_recall_not_highest_to_highest AS recall_not_highest_to_highest,\n",
    "           r.adj_frac_not_highest AS frac_not_highest,\n",
    "           r.adj_frac_highest AS frac_highest, \n",
    "           r.adj_min_score_not_highest AS min_score_not_highest, \n",
    "           r.adj_min_score_highest AS min_score_highest, \n",
    "           r.adj_mean_score_not_highest AS mean_score_not_highest, \n",
    "           r.adj_mean_score_highest AS mean_score_highest, \n",
    "           r.adj_precision_not_highest AS precision_not_highest, \n",
    "           r.adj_precision_highest AS precision_highest, \n",
    "           r.adj_prevalence_not_highest AS prevalence_not_highest, \n",
    "           r.adj_prevalence_highest AS prevalence_highest, \n",
    "           r.adj_positive_prevalence_not_highest AS positive_prevalence_not_highest, \n",
    "           r.adj_positive_prevalence_highest AS positive_prevalence_highest, \n",
    "           r.adj_positive_rate_not_highest AS positive_rate_not_highest, \n",
    "           r.adj_positive_rate_highest AS positive_rate_highest\n",
    "    FROM {schema}.model_adjustment_results_plevel r\n",
    "    JOIN base_mgs b\n",
    "      ON r.model_group_id = b.model_group_id\n",
    "      AND r.past_train_end_time = b.train_end_time\n",
    "      AND r.list_size = b.list_size\n",
    "      AND r.metric = b.metric\n",
    "      AND r.parameter = b.parameter\n",
    "    WHERE r.train_end_time > r.past_train_end_time\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Model selection on last time after adjustment, with adjustment applied\n",
    "    SELECT 'Best Adjusted Metric - Adjusted'::VARCHAR(128) AS strategy,\n",
    "           r.train_end_time, r.past_train_end_time,\n",
    "           r.list_size, r.metric, r.parameter,\n",
    "           r.adj_value AS value,\n",
    "           r.adj_max_recall_ratio AS max_recall_ratio,\n",
    "           r.adj_recall_not_highest_to_highest AS recall_not_highest_to_highest,\n",
    "           r.adj_frac_not_highest AS frac_not_highest,\n",
    "           r.adj_frac_highest AS frac_highest, \n",
    "           r.adj_min_score_not_highest AS min_score_not_highest, \n",
    "           r.adj_min_score_highest AS min_score_highest, \n",
    "           r.adj_mean_score_not_highest AS mean_score_not_highest, \n",
    "           r.adj_mean_score_highest AS mean_score_highest, \n",
    "           r.adj_precision_not_highest AS precision_not_highest, \n",
    "           r.adj_precision_highest AS precision_highest, \n",
    "           r.adj_prevalence_not_highest AS prevalence_not_highest, \n",
    "           r.adj_prevalence_highest AS prevalence_highest, \n",
    "           r.adj_positive_prevalence_not_highest AS positive_prevalence_not_highest, \n",
    "           r.adj_positive_prevalence_highest AS positive_prevalence_highest, \n",
    "           r.adj_positive_rate_not_highest AS positive_rate_not_highest, \n",
    "           r.adj_positive_rate_highest AS positive_rate_highest\n",
    "    FROM {schema}.model_adjustment_results_plevel r\n",
    "    JOIN adj_mgs b\n",
    "      ON r.model_group_id = b.model_group_id\n",
    "      AND r.past_train_end_time = b.train_end_time\n",
    "      AND r.list_size = b.list_size\n",
    "      AND r.metric = b.metric\n",
    "      AND r.parameter = b.parameter\n",
    "    WHERE r.train_end_time > r.past_train_end_time\n",
    "    \n",
    "    UNION ALL\n",
    "\n",
    "    -- Model selection on last time before multiple adjustment, with multiple adjustment applied\n",
    "    SELECT 'Best Unadjusted Metric - MultiAdjusted'::VARCHAR(128) AS strategy,\n",
    "           r.train_end_time, r.past_train_end_time,\n",
    "           r.list_size, r.metric, r.parameter,\n",
    "           r.multi_adj_value AS value,\n",
    "           r.multi_adj_max_recall_ratio AS max_recall_ratio,\n",
    "           r.multi_adj_recall_not_highest_to_highest AS recall_not_highest_to_highest,\n",
    "           r.multi_adj_frac_not_highest AS frac_not_highest,\n",
    "           r.multi_adj_frac_highest AS frac_highest, \n",
    "           r.multi_adj_min_score_not_highest AS min_score_not_highest, \n",
    "           r.multi_adj_min_score_highest AS min_score_highest, \n",
    "           r.multi_adj_mean_score_not_highest AS mean_score_not_highest, \n",
    "           r.multi_adj_mean_score_highest AS mean_score_highest, \n",
    "           r.multi_adj_precision_not_highest AS precision_not_highest, \n",
    "           r.multi_adj_precision_highest AS precision_highest, \n",
    "           r.multi_adj_prevalence_not_highest AS prevalence_not_highest, \n",
    "           r.multi_adj_prevalence_highest AS prevalence_highest, \n",
    "           r.multi_adj_positive_prevalence_not_highest AS positive_prevalence_not_highest, \n",
    "           r.multi_adj_positive_prevalence_highest AS positive_prevalence_highest, \n",
    "           r.multi_adj_positive_rate_not_highest AS positive_rate_not_highest, \n",
    "           r.multi_adj_positive_rate_highest AS positive_rate_highest\n",
    "    FROM {schema}.model_adjustment_results_plevel r\n",
    "    JOIN base_mgs b\n",
    "      ON r.model_group_id = b.model_group_id\n",
    "      AND r.past_train_end_time = b.train_end_time\n",
    "      AND r.list_size = b.list_size\n",
    "      AND r.metric = b.metric\n",
    "      AND r.parameter = b.parameter\n",
    "    WHERE r.train_end_time > r.past_train_end_time\n",
    "    \n",
    "    \n",
    "        UNION ALL\n",
    "\n",
    "    -- Model selection on last time after multiple adjustment, with multiple adjustment applied\n",
    "    SELECT 'Best Multi Adjusted Metric - MultiAdjusted'::VARCHAR(128) AS strategy,\n",
    "           r.train_end_time, r.past_train_end_time,\n",
    "           r.list_size, r.metric, r.parameter,\n",
    "           r.multi_adj_value AS value,\n",
    "           r.multi_adj_max_recall_ratio AS max_recall_ratio,\n",
    "           r.multi_adj_recall_not_highest_to_highest AS recall_not_highest_to_highest,\n",
    "           r.multi_adj_frac_not_highest AS frac_not_highest,\n",
    "           r.multi_adj_frac_highest AS frac_highest, \n",
    "           r.multi_adj_min_score_not_highest AS min_score_not_highest, \n",
    "           r.multi_adj_min_score_highest AS min_score_highest, \n",
    "           r.multi_adj_mean_score_not_highest AS mean_score_not_highest, \n",
    "           r.multi_adj_mean_score_highest AS mean_score_highest, \n",
    "           r.multi_adj_precision_not_highest AS precision_not_highest, \n",
    "           r.multi_adj_precision_highest AS precision_highest, \n",
    "           r.multi_adj_prevalence_not_highest AS prevalence_not_highest, \n",
    "           r.multi_adj_prevalence_highest AS prevalence_highest, \n",
    "           r.multi_adj_positive_prevalence_not_highest AS positive_prevalence_not_highest, \n",
    "           r.multi_adj_positive_prevalence_highest AS positive_prevalence_highest, \n",
    "           r.multi_adj_positive_rate_not_highest AS positive_rate_not_highest, \n",
    "           r.multi_adj_positive_rate_highest AS positive_rate_highest\n",
    "    FROM {schema}.model_adjustment_results_plevel r\n",
    "    JOIN multi_adj_mgs b\n",
    "      ON r.model_group_id = b.model_group_id\n",
    "      AND r.past_train_end_time = b.train_end_time\n",
    "      AND r.list_size = b.list_size\n",
    "      AND r.metric = b.metric\n",
    "      AND r.parameter = b.parameter\n",
    "    WHERE r.train_end_time > r.past_train_end_time\n",
    "    \n",
    "    UNION ALL\n",
    "\n",
    "    -- Model selection on last time after multiple adjustment, with multiple adjustment applied\n",
    "    SELECT 'Best Adjusted Metric - MultiAdjusted'::VARCHAR(128) AS strategy,\n",
    "           r.train_end_time, r.past_train_end_time,\n",
    "           r.list_size, r.metric, r.parameter,\n",
    "           r.multi_adj_value AS value,\n",
    "           r.multi_adj_max_recall_ratio AS max_recall_ratio,\n",
    "           r.multi_adj_recall_not_highest_to_highest AS recall_not_highest_to_highest,\n",
    "           r.multi_adj_frac_not_highest AS frac_not_highest,\n",
    "           r.multi_adj_frac_highest AS frac_highest, \n",
    "           r.multi_adj_min_score_not_highest AS min_score_not_highest, \n",
    "           r.multi_adj_min_score_highest AS min_score_highest, \n",
    "           r.multi_adj_mean_score_not_highest AS mean_score_not_highest, \n",
    "           r.multi_adj_mean_score_highest AS mean_score_highest, \n",
    "           r.multi_adj_precision_not_highest AS precision_not_highest, \n",
    "           r.multi_adj_precision_highest AS precision_highest, \n",
    "           r.multi_adj_prevalence_not_highest AS prevalence_not_highest, \n",
    "           r.multi_adj_prevalence_highest AS prevalence_highest, \n",
    "           r.multi_adj_positive_prevalence_not_highest AS positive_prevalence_not_highest, \n",
    "           r.multi_adj_positive_prevalence_highest AS positive_prevalence_highest, \n",
    "           r.multi_adj_positive_rate_not_highest AS positive_rate_not_highest, \n",
    "           r.multi_adj_positive_rate_highest AS positive_rate_highest\n",
    "    FROM {schema}.model_adjustment_results_plevel r\n",
    "    JOIN adj_mgs b\n",
    "      ON r.model_group_id = b.model_group_id\n",
    "      AND r.past_train_end_time = b.train_end_time\n",
    "      AND r.list_size = b.list_size\n",
    "      AND r.metric = b.metric\n",
    "      AND r.parameter = b.parameter\n",
    "    WHERE r.train_end_time > r.past_train_end_time\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Composite model\n",
    "    SELECT 'Composite Model - Adjusted'::VARCHAR(128) AS strategy,\n",
    "          train_end_time, past_train_end_time,\n",
    "          list_size, metric, parameter,\n",
    "          value,\n",
    "          max_recall_ratio,\n",
    "          recall_not_highest_to_highest,\n",
    "          frac_not_highest,\n",
    "          frac_highest, \n",
    "          min_score_not_highest, \n",
    "          min_score_highest, \n",
    "          mean_score_not_highest, \n",
    "          mean_score_highest, \n",
    "          precision_not_highest, \n",
    "          precision_highest, \n",
    "          prevalence_not_highest, \n",
    "          prevalence_highest, \n",
    "          positive_prevalence_not_highest, \n",
    "          positive_prevalence_highest, \n",
    "          positive_rate_not_highest, \n",
    "          positive_rate_highest\n",
    "    FROM {schema}.composite_results_plevel\n",
    "    WHERE train_end_time > past_train_end_time\n",
    "    ;\n",
    "    \"\"\".format(schema=schema)\n",
    "\n",
    "    ts_df = pd.read_sql(sql, engine)\n",
    "\n",
    "    ts_df['dataset'] = 'Education Crowdfunding'\n",
    "\n",
    "    ts_df['strategy'] = ts_df['strategy'].map({\n",
    "        'Best Unadjusted Metric - Unadjusted': 'Unmitigated', \n",
    "        'Best Adjusted Metric - Adjusted': 'Mitigated - Single Model', \n",
    "        'Best Adjusted Metric - MultiAdjusted': 'MultiMitigated - Adj Seln.',\n",
    "        'Best Multi Adjusted Metric - MultiAdjusted': 'MultiMitigated - MAdj Seln.',\n",
    "        'Composite Model - Adjusted': 'Mitigated - Composite Model',\n",
    "        'Best Unadjusted Metric - Adjusted': 'Mitigated - Unadj. Model Seln.', \n",
    "        'Best Unadjusted Metric - MultiAdjusted': \"MultiMitigated - Unadj. Model Seln.\"        \n",
    "    })\n",
    "    \n",
    "    return ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exhaustive_df(engine, schema=\"bias_results\"):\n",
    "    sql = f\"\"\"\n",
    "    SELECT model_id, model_group_id , train_end_time , past_train_end_time , multi_adj_past_train_end_time , base_frac_highest , adj_frac_highest , multi_adj_frac_highest, base_recall_not_highest_to_highest , adj_recall_not_highest_to_highest , multi_adj_recall_not_highest_to_highest, base_value , adj_value , multi_adj_value  FROM {schema}.model_adjustment_results_plevel marp\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(sql, engine)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing Figures from the Study\n",
    "\n",
    "As a starting point, we can reproduce the figures from the study directly using the data in the `bias_results_submitted` schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df3 = get_bias_results(engine_donors, 'bias_results')\n",
    "print(ts_df3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "months = mdates.MonthLocator([1,3,5,7,9,11])\n",
    "months_fmt = mdates.DateFormatter('%b %y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_plot_over_time(y, y_label, ax=None, strategies=[\"Unmitigated\", \"Mitigated - Single Model\", \"MultiMitigated - Adj Seln.\", \"MultiMitigated - MAdj Seln.\", \"MultiMitigated - Unadj. Model Seln.\"]):\n",
    "    if strategies is None:\n",
    "        df = ts_df3\n",
    "    else:\n",
    "        df = ts_df3.loc[ts_df3[\"strategy\"].isin(strategies), ]\n",
    "    if len(strategies) == 1:\n",
    "        hue = None\n",
    "    else:\n",
    "        hue = \"strategy\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12,5))\n",
    "    sns.lineplot(data=df, x='train_end_time', y=y, hue=hue, marker=\"o\", ax=ax)\n",
    "\n",
    "    ax.set_ylabel(y_label, fontsize=14)\n",
    "    ax.set_xlabel('Test Set Date', fontsize=14)\n",
    "\n",
    "    ax.xaxis.set_major_locator(months)\n",
    "    ax.xaxis.set_major_formatter(months_fmt)\n",
    "\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "\n",
    "    ax.get_legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_plots_over_time(ys, y_labels, ax=None, strategies=[\"Unmitigated\", \"Mitigated - Single Model\"]):\n",
    "    if strategies is None:\n",
    "        df = ts_df3\n",
    "    else:\n",
    "        df = ts_df3.loc[ts_df3[\"strategy\"].isin(strategies), ]    \n",
    "    if len(strategies) == 1:\n",
    "        hue = None\n",
    "    else:\n",
    "        hue = \"strategy\"\n",
    "    markers = [\"*\", \"o\", \"s\", \"^\", \"P\"]\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12,5))\n",
    "    for i, y in enumerate(ys):\n",
    "        y_label = y_labels[i]\n",
    "        marker = markers[i]\n",
    "        sns.lineplot(data=df, x='train_end_time', y=y, hue=hue, marker=marker, ax=ax, label=y_label)\n",
    "    ax.set_xlabel('Test Set Date', fontsize=14)\n",
    "    ax.xaxis.set_major_locator(months)\n",
    "    ax.xaxis.set_major_formatter(months_fmt)\n",
    "\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    \n",
    "    ax.get_legend()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Figure 3\n",
    "Comparing model precision (a) and disparity (b) metrics over time fordifferent model selection strategies in the Education Crowdfunding policy context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "sns.lineplot(data=ts_df3.loc[ts_df3['strategy'] != 'Mitigated - Unadj. Model Seln.', ], x='train_end_time', y='value', hue='strategy', marker=\"o\", ax=ax)\n",
    "\n",
    "ax.set_ylim((0.30,0.70))\n",
    "ax.set_ylabel('Overall Precision at 1000', fontsize=14)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(months_fmt)\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "hhandles = []\n",
    "llabels = []\n",
    "for i, lab in enumerate(labels):\n",
    "    if lab not in ('strategy'):\n",
    "        llabels.append(lab)\n",
    "        hhandles.append(handles[i])\n",
    "ax.legend(hhandles, llabels, ncol=1, fontsize=14, loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_plot_over_time(y=\"recall_not_highest_to_highest\", y_label='Recall Disparity:\\nLower to Higher Poverty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_plot_over_time(y=\"frac_highest\", y_label='Fraction of list set to highest plevel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ts_df3.copy()[~ts_df3[\"strategy\"].isin([\"Unmitigated\"])]\n",
    "grouped = df.groupby(\"strategy\")\n",
    "mean_val = grouped.mean()[\"value\"] \n",
    "std_val = grouped.std()[\"value\"]\n",
    "c_mean_recp = grouped.mean()[\"recall_not_highest_to_highest\"] - 1\n",
    "std_recp = grouped.std()[\"recall_not_highest_to_highest\"]\n",
    "mean_val.plot(kind=\"bar\")\n",
    "plt.title(\"Mean Precision@1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mean_recp.plot(kind=\"bar\")\n",
    "plt.title(\"Mean Recall Parity - 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_recp.plot(kind=\"bar\", )\n",
    "plt.title(\"Stdev Recall Parity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rpt\"] = abs(df[\"recall_not_highest_to_highest\"] - 1)\n",
    "for strat in list(df[\"strategy\"].unique()):\n",
    "    df[df[\"strategy\"] == strat][\"rpt\"].plot(kind=\"hist\")\n",
    "    plt.title(f\"Distribution of Abs(Recall Parity-1) for {strat}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2 (Education Crowdfunding Points)\n",
    "Comparing trade-offs across strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming a few columns to ensure consistency across datasets\n",
    "comp_df = ts_df3.rename(\n",
    "    {'recall_not_highest_to_highest': 'recall_disp', \n",
    "     'frac_not_highest': 'frac_grp1', \n",
    "     'frac_highest': 'frac_grp2'\n",
    "    }, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sty_ord = [\n",
    "    'Mitigated - Single Model', \n",
    "    'Mitigated - Composite Model',\n",
    "    'Mitigated - Unadj. Model Seln.',\n",
    "    'MultiMitigated - Adj Seln.', \n",
    "    'Unmitigated'\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "sns.lineplot(data=comp_df.groupby(['dataset', 'strategy'])[['value', 'recall_disp']].mean().reset_index(), \n",
    "              x='value', y='recall_disp', \n",
    "              hue='dataset', \n",
    "              style='strategy',\n",
    "              style_order=sty_ord,\n",
    "              markers=['X', 'o', 'P', 's'],\n",
    "              markersize=10,\n",
    "              dashes=None,\n",
    "              ci=None,\n",
    "              ax=ax)\n",
    "\n",
    "\n",
    "for i, ds in enumerate(comp_df['dataset'].unique()):\n",
    "    tmp_df = comp_df.loc[comp_df['dataset'] == ds, ].copy()\n",
    "    \n",
    "    x_coords = list(tmp_df.groupby(['dataset', 'strategy'])[['value', 'recall_disp']].mean().reset_index()['value'].values)\n",
    "    y_coords = list(tmp_df.groupby(['dataset', 'strategy'])[['value', 'recall_disp']].mean().reset_index()['recall_disp'].values)\n",
    "\n",
    "    prec_errors = 1.96*tmp_df.groupby(['dataset', 'strategy'])['value'].sem().values\n",
    "    disp_errors = 1.96*tmp_df.groupby(['dataset', 'strategy'])['recall_disp'].sem().values\n",
    "    colors = sns.color_palette().as_hex()[i]\n",
    "    ax.errorbar(x_coords, y_coords, \n",
    "                xerr=prec_errors, \n",
    "                yerr=disp_errors,\n",
    "        ecolor=colors, \n",
    "        fmt=' ', zorder=-1, capsize=5)\n",
    "\n",
    "\n",
    "ax.set_xlim((0.40,0.82))\n",
    "\n",
    "ax.set_ylabel('Recall Disparity', fontsize=16)\n",
    "ax.set_xlabel('Precision at Top k', fontsize=16)\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "hhandles = []\n",
    "llabels = []\n",
    "for i, lab in enumerate(labels):\n",
    "    if lab not in list(comp_df['dataset'].unique()) + ['dataset', 'strategy']:\n",
    "        handles[i].set_linestyle(\"\")\n",
    "    hhandles.append(handles[i])\n",
    "    llabels.append(lab)\n",
    "ax.legend(hhandles, llabels, fontsize=16, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., markerscale=2)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing the Bias Adjustments\n",
    "\n",
    "The following code will re-run the bias analysis itself starting from the model results. Note that there is some randomness in the code (breaking ties between projects with the same score as well as breaking ties between models with the same performance on a given test set), so the results you get here may not be exactly identical to the results above, but should be statistically consistent with them.\n",
    "\n",
    "**NOTE: Running all of the results here can take a fair amount of time (around 2-3 hours on a db.m5.2xlarge AWS RDS instance), so be sure to be working in a screen/tmux setting if running the notebook remotely!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, this code assumes your data is in a postgres database containing modeling results in the format of our open-source `triage` machine learning pipeline, including temporal validation splits.\n",
    "\n",
    "\n",
    "Next, let's clear out anything in the `bias_results` schema (these should start empty, unless you've already run anything since loading the database):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_donors.execute('TRUNCATE TABLE bias_results.composite_results_plevel;')\n",
    "engine_donors.execute('TRUNCATE TABLE bias_results.model_adjustment_results_plevel;')\n",
    "engine_donors.execute('TRUNCATE TABLE bias_working.model_adjustment_group_k_plevel;')\n",
    "engine_donors.execute('COMMIT;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DJRecallAdjuster import education_ra_procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.99, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "education_ra_procedure(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to explore the results further, `bias_results.model_adjustment_results_plevel` holds the results from your run for each model, including the validation set the thresholds were learned based on (`past_train_end_time`), the validation set they were applied to (`train_end_time`), and the unadjusted (`base_` columns) and adjusted (`adj_` columns) results in terms of accuracy, disparities, and the composition of the selected list. The `bias_results.composite_results_plevel` has a similar structure for the composite models created by selecting the best-performing model for each subgroup separately (so note there's no overall `model_id` ehre, but separate columns for the model group used for each subgroup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT * FROM bias_results.model_adjustment_results_plevel LIMIT 5\", engine_donors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3env",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
